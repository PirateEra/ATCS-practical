{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file was only used, to test my works during the creation of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/ATCS/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import numpy as np\n",
    "from utils import preprocess_data, build_vocab, create_embedding_dict, encode_example, create_model_data_rep, create_batch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # this one was needed for the tokenizer of nltk (pretrained model, to properly tokenize aka punkt)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 35044.88 examples/s]\n",
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 43555.64 examples/s]\n",
      "Filter: 100%|██████████| 550152/550152 [00:11<00:00, 46919.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#type(dataset)\n",
    "preprocessed_data = preprocess_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(preprocessed_data):\n",
    "    vocab = set()\n",
    "    for split in preprocessed_data:\n",
    "        for example in preprocessed_data[split]:\n",
    "            vocab.update(example[\"premise\"])\n",
    "            vocab.update(example[\"hypothesis\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vocab = build_vocab(preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.840B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0] # A word such as dog\n",
    "            if word in unique_vocab: # Only take in account the words that appear in the SNLI dataset, to speed up training\n",
    "                vector = np.asarray(values[1:], \"float32\") # The embedding vector of the word\n",
    "                embeddings_dict[word] = vector # A dict key-value of word/vector\n",
    "        except:\n",
    "            # Some cases in glove seem to be messed up, such as . . . or at name@domain.com\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the dataset (tokenized) with the glove embeddings to get the glove embedding of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_glove(tokens, glove_dict, dim=300):\n",
    "    vectors = [glove_dict.get(token, np.zeros(dim)) for token in tokens]\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(example):\n",
    "    return {\n",
    "        \"premise_glove\": sentence_to_glove(example[\"premise\"], embeddings_dict),\n",
    "        \"hypothesis_glove\": sentence_to_glove(example[\"hypothesis\"], embeddings_dict),\n",
    "        \"label\": example[\"label\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocessed_data[\"train\"]\n",
    "train_example = encode_example(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.']\n",
      "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data['premise'][0])\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "1000\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_data[\"train\"]))\n",
    "print(len(preprocessed_data[\"train\"].select(range(1000))))\n",
    "print(type(preprocessed_data[\"train\"]))\n",
    "print(preprocessed_data[\"train\"].select(range(1000))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_model_data_rep(preprocessed_data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[490443 339932 258008 ... 439308 502453 228306]\n",
      "Min target label: 0\n",
      "Max target label: 2\n"
     ]
    }
   ],
   "source": [
    "train_rows = len(train_data['premise'])\n",
    "\n",
    "# Shuffle the data and get the premises, hypothesis and target_labels (shuffled) for the entire training set\n",
    "random_indices = np.random.permutation(train_rows)\n",
    "premises = train_data['premise'][random_indices]\n",
    "hypothesis = train_data['hypothesis'][random_indices]\n",
    "target_labels = train_data['label'][random_indices]\n",
    "print(\"Min target label:\", target_labels.min())\n",
    "print(\"Max target label:\", target_labels.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'boy', 'is', 'standing', 'next', 'to', 'a', 'car', 'in', 'front', 'of', 'a', 'clothesline', '.']\n",
      "['the', 'boy', 'is', 'by', 'a', 'clothesline', '.']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(premises[0])\n",
    "print(hypothesis[0])\n",
    "print(target_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_batch = create_batch(premises[0:64], embeddings_dict)\n",
    "hypothesis_batch = create_batch(hypothesis[0:64], embeddings_dict)\n",
    "target_batch = torch.LongTensor(target_labels[0:64].astype(int))\n",
    "#target_batch = create_batch(target_labels[0:64], embeddings_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
